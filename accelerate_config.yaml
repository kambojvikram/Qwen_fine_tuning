# accelerate_config.yaml
compute_environment: LOCAL_MACHINE
distributed_type: DEEPSPEED
downcast_bf16: 'no'
machine_rank: 0
main_training_function: main
mixed_precision: bf16
num_machines: 1
num_processes: 4
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false

deepspeed_config:
  gradient_accumulation_steps: 8
  gradient_clipping: 1.0
  offload_optimizer_device: cpu
  offload_param_device: cpu
  zero3_init_flag: true
  zero3_save_16bit_model: true
  zero_stage: 3

---
# training_config.json - Alternative JSON configuration
{
  "model_args": {
    "model_name_or_path": "Qwen/Qwen3-32B",
    "trust_remote_code": true,
    "torch_dtype": "bfloat16",
    "attn_implementation": "flash_attention_2",
    "use_flash_attn": true
  },
  "data_args": {
    "dataset_path": "./data/training_data.csv",
    "validation_split": 0.05,
    "max_seq_length": 8192,
    "preprocessing_num_workers": 8
  },
  "training_args": {
    "output_dir": "./qwen3-32b-finetuned",
    "num_train_epochs": 3,
    "per_device_train_batch_size": 1,
    "per_device_eval_batch_size": 1,
    "gradient_accumulation_steps": 8,
    "gradient_checkpointing": true,
    "learning_rate": 5e-5,
    "lr_scheduler_type": "cosine",
    "warmup_ratio": 0.1,
    "weight_decay": 0.01,
    "logging_steps": 10,
    "save_steps": 500,
    "eval_steps": 500,
    "evaluation_strategy": "steps",
    "save_strategy": "steps",
    "save_total_limit": 3,
    "load_best_model_at_end": true,
    "metric_for_best_model": "eval_loss",
    "greater_is_better": false,
    "bf16": true,
    "tf32": true,
    "dataloader_num_workers": 4,
    "remove_unused_columns": false,
    "optim": "adamw_torch",
    "seed": 42,
    "ddp_timeout": 7200,
    "report_to": ["tensorboard"]
  }
}
